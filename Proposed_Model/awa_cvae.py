# -*- coding: utf-8 -*-
"""AWA_CVAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sVGUF6VBVV29T26bmtZAuHJPUyyBiZJz
"""

!wget http://www.sal.ipg.pt/user/1012390/datasets/AWA2.zip

# Extract zip to /content directory

from zipfile import ZipFile

filename = 'AWA2.zip'

with ZipFile(filename, 'r') as zip:
  zip.extractall()
  print('Done')

import numpy as np
import matplotlib.pyplot as plt
import tensorflow.compat.v1 as tf
tf.compat.v1.disable_v2_behavior()
import math
import time
import bz2
import _pickle as cPickle
from sklearn import preprocessing
import numpy as np
import pickle

###################################################################
# UTILS
###################################################################

def decompress_pickle(file):
    """
    Load any compressed pickle file
    :param file: filename
    :return: data inside the compressed pickle file
    """
    data = bz2.BZ2File(file, "rb")
    data = cPickle.load(data)

    return data


def normalize(x):
    """
    Normalize input features
    :param x: original features
    :return: normalized features
    """
    x_norm = x.astype('float32') / 255.0

    return x_norm


def random_mini_batches(X, S, mini_batch_size = 64, seed = 0):
    """
    Creates a list of random minibatches from (X, Y)
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    mini_batch_size -- size of the mini-batches, integer
    
    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """
    
    np.random.seed(seed)            # To make your "random" minibatches the same as ours
    m = X.shape[0]                  # number of training examples
    mini_batches = []
        
    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[permutation, :]
    shuffled_Y = S[permutation, :]

    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[k*mini_batch_size : (k+1)*mini_batch_size, :]
        mini_batch_Y = shuffled_Y[k*mini_batch_size : (k+1)*mini_batch_size, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[(m - mini_batch_size * num_complete_minibatches) : m, :]
        mini_batch_Y = shuffled_Y[(m - mini_batch_size * num_complete_minibatches) : m, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches


def get_all_classes():
    """
    Get all classes
    """
    # Get all classes
    classes = {}
    with open('/content/AWA2/Animals_with_Attributes2/classes.txt') as f_classes:
        lines = f_classes.readlines()
        for l in lines:
            classes[l.strip().split("\t")[1]] = l.strip().split("\t")[0]

    # Get training classes
    train_classes = []
    with open("/content/AWA2/Animals_with_Attributes2/trainclasses1.txt") as f_tclasses:
        lines = f_tclasses.readlines()
        for l in lines:
            classname = l.strip()
            train_classes.append(int(classes[classname]))
    train_classes = np.array(train_classes)

    # Get val classes
    val_classes = []
    with open("/content/AWA2/Animals_with_Attributes2/valclasses1.txt") as f_tclasses:
        lines = f_tclasses.readlines()
        for l in lines:
            classname = l.strip()
            val_classes.append(int(classes[classname]))
    val_classes = np.array(val_classes)

    # Get test classes
    test_classes = []
    with open("/content/AWA2/Animals_with_Attributes2/testclasses_ps.txt") as f:
        lines = f.readlines()
        for l in lines:
            classname = l.strip()
            test_classes.append(int(classes[classname]))
    test_classes = np.array(test_classes)

    return train_classes, val_classes, test_classes


def get_labels():
    labels = np.loadtxt('/content/AWA2/Animals_with_Attributes2/Features/ResNet101/AwA2-labels.txt')

    return labels

def get_class_embeddings(attributes='continuous'):

    if attributes == 'continuous':
        S = np.loadtxt('/content/AWA2/Animals_with_Attributes2/predicate-matrix-continuous.txt')
    elif attributes == 'binary':
        S = np.loadtxt('/content/AWA2/Animals_with_Attributes2/predicate-matrix-binary.txt')

    return S

"""
###############################################################
# Preprocessing data
###############################################################

train_classes, val_classes, test_classes = get_all_classes()
labels = get_labels()
S = get_class_embeddings(attributes='continuous')

# Y Labels
lbl = preprocessing.LabelEncoder()
y_train_orig = labels[np.where([labels == i for i in train_classes])[1]]
y_train = lbl.fit_transform(y_train_orig)
y_test_orig = labels[np.where([labels == i for i in test_classes])[1]]
y_test = lbl.fit_transform(y_test_orig)
y_val_orig = labels[np.where([labels == i for i in val_classes])[1]]
y_val = lbl.fit_transform(y_val_orig)

# X Features
X_train = X[np.where([labels == i for i in train_classes])[1]]
X_test = X[np.where([labels == i for i in test_classes])[1]]
X_val = X[np.where([labels == i for i in val_classes])[1]]

# S Class Embeddings
S_train = S[[int(i - 1) for i in y_train_orig]]
S_test = S[[int(i - 1) for i in y_test_orig]]
S_val = S[[int(i - 1) for i in y_val_orig]]

# Normalize input data
x_train = preprocessing.normalize(X_train, norm='l2')
x_val = preprocessing.normalize(X_val, norm='l2')
x_test = preprocessing.normalize(X_test, norm='l2')
s_train = preprocessing.normalize(S_train, norm='l2')
s_val = preprocessing.normalize(S_val, norm='l2')
s_test = preprocessing.normalize(S_test, norm='l2')

print("[INFO]: Input shapes")
print("x_train: ", x_train.shape)
print("y_train: ", y_train.shape)
print("x_val: ", x_val.shape)
print("y_val: ", y_val.shape)
print("x_test: ", x_test.shape)
print("y_test: ", y_test.shape)
print("s_train: ", s_train.shape)
print("s_val: ", s_val.shape)
print("s_test: ", s_test.shape)
"""

from AWA2.awa import load_awa2_data

awa_data = load_awa2_data(attributes='continuous', split='ps')

# Training data

X_train = awa_data['X_train']
y_train = awa_data['y_train']
# Semantic attributes
s_train_per_class = awa_data['S_train_per_class']
s_train_per_sample = awa_data['S_train_per_sample']

# Validation data

X_val = awa_data['X_val']
y_val = awa_data['y_val']
# Semantic attributes
s_val_per_class = awa_data['S_val_per_class']
s_val_per_sample = awa_data['S_val_per_sample']

# Test data
X_test = awa_data['X_test']
y_test = awa_data['y_test']
s_test_per_class = awa_data['S_test_per_class']
s_test_per_sample = awa_data['S_test_per_sample']

# Normalize input data
x_train = preprocessing.normalize(X_train, norm='l2')
x_val = preprocessing.normalize(X_val, norm='l2')
x_test = preprocessing.normalize(X_test, norm='l2')
s_train = preprocessing.normalize(s_train_per_sample, norm='l2')
s_val = preprocessing.normalize(s_val_per_sample, norm='l2')
s_test = preprocessing.normalize(s_test_per_sample, norm='l2')

print("[INFO]: AWA2 data shapes")
print("x_train: ", X_train.shape)
print("y_train: ", y_train.shape)
print("s_train_per_class: ", s_train_per_class.shape)
print("s_train_per_sample: ", s_train_per_sample.shape)
print("x_val: ", X_val.shape)
print("y_val: ", y_val.shape)
print("s_val_per_class: ", s_val_per_class.shape)
print("s_val_per_sample: ", s_val_per_sample.shape)
print("x_test: ", X_test.shape)
print("y_test: ", y_test.shape)
print("s_test_per_class: ", s_test_per_class.shape)
print("s_test_per_sample: ", s_test_per_sample.shape)

# Parameters for the neural network
learning_param = 0.001
epochs = 100
batch_size = 128
patience = 2

# Network parameters
feat_dimension = 2048
class_embeddings = 85
neural_network_dimension_hidden_encoder = 1560  
neural_network_dimension_hidden_decoder = 1660 

# Change this value for latent space effect on visualization of reconstructed data
latent_variable_dimension = 64

# Inicialization
def xavier(in_shape):
  val = tf.random_normal(shape=in_shape, stddev=1./tf.sqrt(in_shape[0]/2.))
  return val

# Weights and bias dictionary
Weight = {
    "weight_matrix_encoder_hidden": tf.Variable(xavier([feat_dimension + class_embeddings, neural_network_dimension_hidden_encoder])),
    "weight_mean_hidden": tf.Variable(xavier([neural_network_dimension_hidden_encoder, latent_variable_dimension])),
    "weight_std_hidden": tf.Variable(xavier([neural_network_dimension_hidden_encoder, latent_variable_dimension])),
    "weight_matrix_decoder_hidden": tf.Variable(xavier([latent_variable_dimension + class_embeddings, neural_network_dimension_hidden_decoder])),
    "weight_decoder": tf.Variable(xavier([neural_network_dimension_hidden_decoder, feat_dimension]))
}

Bias = {
    "bias_matrix_encoder_hidden": tf.Variable(xavier([neural_network_dimension_hidden_encoder])),
    "bias_mean_hidden": tf.Variable(xavier([latent_variable_dimension])),
    "bias_std_hidden": tf.Variable(xavier([latent_variable_dimension])),
    "bias_matrix_decoder_hidden": tf.Variable(xavier([neural_network_dimension_hidden_decoder])),
    "bias_decoder": tf.Variable(xavier([feat_dimension]))
}

# Building the Variational AutoEncoder

# ENCODER
image_X = tf.placeholder(tf.float32, shape = [None, feat_dimension])
label_X = tf.placeholder(tf.float32, shape = [None, class_embeddings])

modified_image = tf.concat(axis=1, values=[image_X, label_X])
print(modified_image.shape)

encoder_layer = tf.add(tf.matmul(modified_image, Weight['weight_matrix_encoder_hidden']), Bias['bias_matrix_encoder_hidden'])
encoder_layer = tf.nn.tanh(encoder_layer)

mean_layer = tf.add(tf.matmul(encoder_layer, Weight['weight_mean_hidden']), Bias['bias_mean_hidden'])
std_layer = tf.add(tf.matmul(encoder_layer, Weight['weight_std_hidden']), Bias['bias_std_hidden'])

# Reparametrization trick
epsilon = tf.random_normal(tf.shape(std_layer), mean=0.0, stddev=1.0)
latent_layer = mean_layer + tf.exp(0.5*std_layer) * epsilon

# DECODER
modified_in_between = tf.concat(axis=1, values=[latent_layer, label_X])

decoder_hidden = tf.add(tf.matmul(modified_in_between, Weight['weight_matrix_decoder_hidden']), Bias['bias_matrix_decoder_hidden'])
decoder_hidden = tf.nn.tanh(decoder_hidden)

decoder_output = tf.add(tf.matmul(decoder_hidden, Weight['weight_decoder']), Bias['bias_decoder'])
decoder_output = tf.nn.sigmoid(decoder_output)

def loss_function(reconstructed_image, Y):

  # Reconstruction loss
  #data_fidelity_loss = Y*tf.log(1e-10 + reconstructed_image) + (1-Y)*tf.log(1e-10 + 1 - reconstructed_image)
  data_fidelity_loss = tf.losses.mean_squared_error(Y, reconstructed_image)

  # KL Divergence loss
  kl_div_loss = 1 + std_layer - tf.square(mean_layer) - tf.exp(std_layer)
  kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, 1)

  # Depending on witch loss you have to take, select values of alpha and beta
  # If only KL loss is to be evaluated, put alpha = 0. If fidelity loss, put beta = 0. For VAE, both should be 1
  alpha = 1
  beta = 1
  network_loss = tf.reduce_mean(alpha*data_fidelity_loss + beta*kl_div_loss)

  return network_loss

loss_value = loss_function(decoder_output, image_X)
optimizer = tf.train.RMSPropOptimizer(learning_param).minimize(loss_value)

# Initialize all variables
init = tf.global_variables_initializer()

# Executing the computational graph

# Start the session
sess = tf.Session()

# Run the initializer
sess.run(init)

train_loss = []
validation_loss = []
for i in range(epochs):
  
  minibatches = random_mini_batches(x_train, s_train, mini_batch_size=batch_size)

  loss_train = 0
  for batch in minibatches:
    (minibatch_X, minibatch_Y) = batch
    _, loss = sess.run([optimizer, loss_value], feed_dict={image_X : minibatch_X, label_X : minibatch_Y})
    loss_train += loss
  
  loss_train /= len(minibatches) 
  train_loss.append(loss_train)
  
  # Validation
  minibatches_val = random_mini_batches(x_val, s_val, mini_batch_size=batch_size)

  loss_val = 0
  for batch in minibatches_val:
    (minibatch_X, minibatch_Y) = batch
    _, val_loss = sess.run([optimizer, loss_value], feed_dict={image_X : minibatch_X, label_X : minibatch_Y})
    loss_val += loss

  loss_val /= len(minibatches_val)
  validation_loss.append(loss_val)

  print('Epoch {0} :: loss: {1} - val_loss: {2}'.format(i, loss_train, loss_val))

  if patience > 0:
    last_difs = np.diff(np.asarray(validation_loss))
    if len(last_difs) >= patience:
      if (last_difs[-patience:] > 0).all():
        break

########################################################################################################################
# Save model
########################################################################################################################
import os
import datetime
saver = tf.train.Saver()
saver.save(sess, 'model')

# Plot training and validation loss
plt.figure(figsize=(8,10))
plt.plot(range(len(train_loss)), train_loss, label='Training Loss')
plt.plot(range(len(train_loss)), validation_loss, label='Validation Loss')
plt.legend(loc='lower right')
plt.title('Training and Validation Loss')
plt.show()

#plt.savefig(os.path.join(date_time_folder, 'Learning.png'))

# Testing phase

# Noisy input handle
noise_X = tf.placeholder(tf.float32, shape = [None, latent_variable_dimension])

# Rebuild the decoder to create the output image from noise

# DECODER

modified_in_between = tf.concat(axis=1, values=[noise_X, label_X])

decoder_hidden = tf.add(tf.matmul(modified_in_between, Weight['weight_matrix_decoder_hidden']), Bias['bias_matrix_decoder_hidden'])
decoder_hidden = tf.nn.tanh(decoder_hidden)

decoder_output = tf.add(tf.matmul(decoder_hidden, Weight['weight_decoder']), Bias['bias_decoder'])
decoder_output = tf.nn.sigmoid(decoder_output)

# Output visualization
train_classes, val_classes, test_classes = get_all_classes()
labels = get_labels()
S = get_class_embeddings(attributes='continuous')
S_test_orig = S[[(i - 1) for i in test_classes]]
S_test = preprocessing.normalize(S_test_orig, norm='l2')

# Number of features to generate
n_gen = 750

class_emb_gen = {}
for i in range(len(S_test)):
  x_label = np.array([S_test[i]]*n_gen)

  generated_latent_layer = np.random.normal(0,1,size=[n_gen, latent_variable_dimension])
  generated_image_features = sess.run(decoder_output, feed_dict={noise_X : generated_latent_layer, label_X : x_label})
  class_emb_gen[i] = generated_image_features

# Save generated features as a pickle file
filename = "class_emb_gen_30nov2020.pkl"
pickle.dump(class_emb_gen, open(filename, "wb"))

test_classes_names = ['sheep', 'dolphin', 'bat', 'seal', 'blue+whale', 'rat', 'horse', 'walrus', 'giraffe', 'bobcat']

# generated features
n = 10
plt.figure(figsize=(20, 8))
for i in range(1, n + 1):
    ax = plt.subplot(1, n, i)
    plt.imshow(class_emb_gen[i-1][1].reshape((16, 16 * 8)).T)
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    plt.title(str(test_classes_names[i-1]))
plt.show()

# original features
idxs = [0, 2000, 2500, 3000, 3900, 4000, 5000, 6000, 7000, 7500]
n = 10
plt.figure(figsize=(20, 8))
for i in range(1, n + 1):
    ax = plt.subplot(1, n, i)
    plt.imshow(x_test[idxs[i-1]].reshape((16, 16 * 8)).T)
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    plt.title(str(test_classes_names[i-1]))
plt.show()