# -*- coding: utf-8 -*-
"""softmax_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JgAHljKzQA7d7XdYquZVyDg4e8W7qYfY
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import math
import time
import bz2
import _pickle as cPickle
import pickle
from sklearn import preprocessing

!wget http://www.sal.ipg.pt/user/1012390/datasets/AWA2.zip

# Extract zip to /content directory

from zipfile import ZipFile

filename = 'AWA2.zip'

with ZipFile(filename, 'r') as zip:
  zip.extractall()
  print('Done')

###########################################################################
# UTILS
###########################################################################

def decompress_pickle(file):
    """
    Load any compressed pickle file
    :param file: filename
    :return: data inside the compressed pickle file
    """
    data = bz2.BZ2File(file, "rb")
    data = cPickle.load(data)
 
    return data
 
 
def read_pickle(file):
    data = pickle.load(open(file, "rb"))
 
    return data
 
 
def normalize(x):
    """
    Normalize input features
    :param x: original features
    :return: normalized features
    """
    x_norm = x.astype('float32') / 255.0
 
    return x_norm
 
def get_all_classes():
    """
    Get all classes
    """
    # Get all classes
    classes = {}
    with open('/content/AWA2/Animals_with_Attributes2/classes.txt') as f_classes:
        lines = f_classes.readlines()
        for l in lines:
            classes[l.strip().split("\t")[1]] = l.strip().split("\t")[0]
 
    # Get training classes
    train_classes = []
    with open("/content/AWA2/Animals_with_Attributes2/trainclasses1.txt") as f_tclasses:
        lines = f_tclasses.readlines()
        for l in lines:
            classname = l.strip()
            train_classes.append(int(classes[classname]))
    train_classes = np.array(train_classes)
 
    # Get test classes
    test_classes = []
    with open("/content/AWA2/Animals_with_Attributes2/testclasses_ps.txt") as f:
        lines = f.readlines()
        for l in lines:
            classname = l.strip()
            test_classes.append(int(classes[classname]))
    test_classes = np.array(test_classes)
 
    # Get test classes
    test_classes_all = []
    with open("/content/AWA2/Animals_with_Attributes2/testclasses.txt") as f:
        lines = f.readlines()
        for l in lines:
            classname = l.strip()
            test_classes_all.append(int(classes[classname]))
    test_classes_all = np.array(test_classes_all) 
 
 
    return train_classes, test_classes, test_classes_all
 
 
def get_labels():
    labels = np.loadtxt('/content/AWA2/Animals_with_Attributes2/Features/ResNet101/AwA2-labels.txt')
 
    return labels
 
def get_class_embeddings(attributes='continuous'):
 
    if attributes == 'continuous':
        S = np.loadtxt('/content/AWA2/Animals_with_Attributes2/predicate-matrix-continuous.txt')
    elif attributes == 'binary':
        S = np.loadtxt('/content/AWA2/Animals_with_Attributes2/predicate-matrix-binary.txt')
 
    return S

################################################################
# Preprocessing data
###############################################################
 
from AWA2.awa import load_awa2_data

awa_data = load_awa2_data(attributes='continuous', split='ps')

# Training data
X_train = awa_data['X_train']
y_train = awa_data['y_train']
s_train_per_sample = awa_data['S_train_per_sample']

# Validation data
X_val = awa_data['X_val']
y_val = awa_data['y_val']
s_val_per_sample = awa_data['S_val_per_sample']

# Test data
X_test = awa_data['X_test']
y_test = awa_data['y_test']
s_test_per_sample = awa_data['S_test_per_sample']

# Normalize input data
x_train = preprocessing.normalize(X_train, norm='l2')
x_val = preprocessing.normalize(X_val, norm='l2')
x_test = preprocessing.normalize(X_test, norm='l2')
s_train = preprocessing.normalize(s_train_per_sample, norm='l2')
s_val = preprocessing.normalize(s_val_per_sample, norm='l2')
s_test = preprocessing.normalize(s_test_per_sample, norm='l2')

# Gen features
n_feat = 750
gen_X = read_pickle('class_emb_gen_30nov2020.pkl')
train_classes, val_classes, test_classes = get_all_classes()
print(test_classes)
lbl = preprocessing.LabelEncoder()
y_gen = [[i]*n_feat for i in test_classes]
y_gen = np.asarray(y_gen).flatten()
y_gen = lbl.fit_transform(y_gen)

X_gen = []
for key, value in gen_X.items():
  for i in range(n_feat):
    X_gen.append(value[i])
X_gen = np.asarray(X_gen)

lbl = preprocessing.LabelEncoder()
x_train_final = np.concatenate((x_train, X_gen), axis=0)
y_train_final = np.concatenate((y_train, y_gen), axis=0)
#y_train_final = lbl.fit_transform(y_train_final)
 
print("[INFO]: AWA2 data shapes")
print("x_train: ", X_train.shape)
print("y_train: ", y_train.shape)
print("s_train_per_sample: ", s_train_per_sample.shape)
print("x_val: ", X_val.shape)
print("y_val: ", y_val.shape)
print("s_val_per_sample: ", s_val_per_sample.shape)
print("x_test: ", X_test.shape)
print("y_test: ", y_test.shape)
print("s_test_per_sample: ", s_test_per_sample.shape)
print("x_gen: ", X_gen.shape)
print("y_gen: ", y_gen.shape)
print("x_train_final: ", x_train_final.shape)
print("y_train_final: ", y_train_final.shape)

model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(512, activation = tf.nn.relu)  
      tf.keras.layers.Dense(len(np.unique(y_train_final)), activation = tf.nn.softmax)
])
print(len(np.unique(y_train_final)))

model.compile(optimizer = tf.optimizers.Adam(), 
              loss = 'sparse_categorical_crossentropy', 
              metrics =['accuracy']) 

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3)
  
model.fit(x_train_final, y_train_final, 
          epochs = 50,
          validation_data=(X_val, y_val),
          callbacks=[early_stopping])

preds = np.argmax(model.predict(x_test), axis=1) 
 
from sklearn.metrics import accuracy_score
 
print("Accuracy (unseen): %.3f %%" % (accuracy_score(y_test, preds)))